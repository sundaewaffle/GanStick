{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ConditionalDCGAN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Q5ky7Mm8L5bKWbV_cEHep7F1tzo00src","authorship_tag":"ABX9TyPxJieKZIwAfukRVdTgNVfC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6wpqQoFhBYkN"},"source":["!pip install imageio\n","!pip install git+https://github.com/tensorflow/docs\n","!pip install XlsxWriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejOpmsdPBhlx"},"source":["import tensorflow as tf\n","import glob\n","import imageio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","import tensorflow.keras as keras\n","from tensorflow.keras import layers\n","import time\n","\n","from IPython import display\n","import tensorflow_docs.vis.embed as embed\n","import xlsxwriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xx2MCiBCBjpA"},"source":["BATCH_SIZE = 256\n","\n","historical_images = tf.keras.preprocessing.image_dataset_from_directory(\n","    'drive/MyDrive/GanStick/dataset_eq/historical',\n","    batch_size=BATCH_SIZE,\n","    image_size=(56,56),\n","    shuffle=False,\n","    labels=None,\n","    smart_resize=True\n",")\n","\n","future_images = tf.keras.preprocessing.image_dataset_from_directory(\n","    'drive/MyDrive/GanStick/dataset_eq/future',\n","    batch_size=BATCH_SIZE,\n","    image_size=(56,56),\n","    shuffle=False,\n","    labels=None,\n","    smart_resize=True\n",")\n","\n","def process(image):\n","  image = tf.cast((image-127.5) / 127.5 ,tf.float32)\n","  return image\n","\n","historical_dataset = historical_images.map(process)\n","future_dataset = future_images.map(process)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wxwct-qVCAx1"},"source":["def make_generator_model():\n","  gen_input_image = layers.Input(shape=(56, 56, 3))\n","  gen_input_noise = layers.Input(shape=(100,))\n","\n","  image = layers.Conv2D(128, (5,5), strides=(2,2), padding='same', input_shape=[56, 56, 3])(gen_input_image)\n","  image = layers.BatchNormalization()(image)\n","  image = layers.LeakyReLU()(image)\n","\n","  image = layers.Conv2D(64, (4,4), strides=(2,2), padding='same')(image)\n","  image = layers.BatchNormalization()(image)\n","  image = layers.LeakyReLU()(image)\n","\n","  image = layers.Conv2D(32, (2,2), strides=(2,2), padding='same')(image)\n","  image = layers.BatchNormalization()(image)\n","  image = layers.LeakyReLU()(image)\n","\n","  image = layers.Flatten()(image)\n","  image = layers.Dense(64)(image)\n","  image = layers.BatchNormalization()(image)\n","  image = layers.LeakyReLU()(image)\n","\n","  image = layers.Dense(32)(image)\n","  image = layers.BatchNormalization()(image)\n","  image = layers.LeakyReLU()(image)\n","  image = layers.Dense(10, activation=tf.nn.tanh)(image)\n","\n","  noise = layers.Concatenate(axis=1)([image, gen_input_noise])\n","  noise = layers.Dense(7*7*256, use_bias=False, input_shape=(110,))(noise)\n","  noise = layers.BatchNormalization()(noise)\n","  noise = layers.LeakyReLU()(noise)\n","\n","  gen_image = layers.Reshape((7, 7, 256))(noise)\n","  \n","  gen_image = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(gen_image)\n","  gen_image = layers.BatchNormalization()(gen_image)\n","  gen_image = layers.LeakyReLU()(gen_image)\n","\n","  gen_image = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(gen_image)\n","  gen_image = layers.BatchNormalization()(gen_image)\n","  gen_image = layers.LeakyReLU()(gen_image)\n","\n","  gen_image = layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)(gen_image)\n","  gen_image = layers.BatchNormalization()(gen_image)\n","  gen_image = layers.LeakyReLU()(gen_image)\n","\n","  gen_image = layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh')(gen_image)\n","  \n","  model = keras.Model(inputs=[gen_input_image, gen_input_noise], outputs=gen_image)\n","\n","  return model\n","\n","def make_discriminator_model():\n","  model = tf.keras.Sequential()\n","  model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[56, 56, 3]))\n","  model.add(layers.LeakyReLU())\n","  model.add(layers.Dropout(0.3))\n","\n","  model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n","  model.add(layers.LeakyReLU())\n","  model.add(layers.Dropout(0.3))\n","\n","  model.add(layers.Flatten())\n","  model.add(layers.Dense(1))\n","\n","  return model\n","\n","generator = make_generator_model()\n","generator.summary()\n","discriminator = make_discriminator_model()\n","discriminator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gA2z8UzEGKvt"},"source":["cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","generator_optimizer = tf.keras.optimizers.Adam(0.0002)\n","discriminator_optimizer = tf.keras.optimizers.Adam(0.0002)\n","\n","def generator_loss(fake_output):\n","  return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","def discriminator_loss(real_output, fake_output):\n","  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","  total_loss = real_loss + fake_loss\n","  return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylzUOedAK-dR"},"source":["checkpoint_dir = 'drive/MyDrive/GanStick/ConditionalDCGAN_check'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                  discriminator_optimizer=discriminator_optimizer,\n","                  generator=generator,\n","                  discriminator=discriminator)\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFEE-z5dLZN4"},"source":["EPOCHS = 90\n","noise_dim = 100\n","\n","@tf.function\n","def train_step(real_images, historical_images):\n","  current_batch_size = historical_images.shape[0]\n","  noise = tf.random.normal([current_batch_size, noise_dim])\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    generated_images = generator([historical_images, noise], training=True)\n","\n","    real_output = discriminator(real_images, training=True)\n","    fake_output = discriminator(generated_images, training=True)\n","\n","    gen_loss = generator_loss(fake_output)\n","    disc_loss = discriminator_loss(real_output, fake_output)\n","\n","  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","  return gen_loss, disc_loss, real_images, generated_images, fake_output\n","\n","\n","def train(future_dataset, historical_dataset, epochs, last_epoch):\n","  for epoch in range(epochs):\n","    start = time.time()\n","    recent_epoch = epoch + last_epoch + 1\n","    data = []\n","    real_images = []\n","    generated_images = []\n","    batch_num = 0\n","    print('Start training for epoch {}'.format(recent_epoch))\n","\n","    for future_images, historical_images in zip(future_dataset, historical_dataset):\n","      gen_loss, disc_loss, real_image, generated_image, fake_output = train_step(future_images, historical_images)\n","      data.append((gen_loss.numpy(), disc_loss.numpy(), fake_output.numpy()[0]))\n","      real_images.append(real_image.numpy()[0])\n","      generated_images.append(generated_image.numpy()[0])\n","      batch_num += 1\n","      if (batch_num % 10) == 0:\n","        print('Batch {} training finished'.format(batch_num))\n","    \n","    if (recent_epoch) % 10 == 0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    save_result(data, real_images, generated_images, recent_epoch)\n","    display.clear_output(wait=True)\n","    print ('Time for epoch {} is {} sec'.format(recent_epoch, time.time()-start))\n","    print ('generator loss:', gen_loss.numpy())\n","    print ('disciminator loss:', disc_loss.numpy())\n","\n","\n","def save_result(data, real_images, generated_images, epoch_num):\n","  wb = xlsxwriter.Workbook(f'drive/MyDrive/GanStick/ConditionalDCGAN_result/epoch{epoch_num:03}.xlsx')\n","  os.makedirs(f'drive/MyDrive/GanStick/ConditionalDCGAN_result/epoch{epoch_num:03}/real', exist_ok=True)\n","  os.makedirs(f'drive/MyDrive/GanStick/ConditionalDCGAN_result/epoch{epoch_num:03}/generated', exist_ok=True)\n","  ws = wb.add_worksheet()\n","  ws.write_row(0, 0, ('Batch Index', 'Generator Loss', 'Discriminator Loss', 'Generated Image Prediction'))\n","  batch_num = 1\n","  for result, real_img, gene_img in zip(data, real_images, generated_images):\n","    ws.write_row(batch_num, 0, (batch_num, result[0], result[1], result[2]))\n","    save_img = (real_img * 127.5 + 127.5)\n","    save_img = PIL.Image.fromarray(np.uint8(save_img))\n","    save_img.save(f'drive/MyDrive/GanStick/ConditionalDCGAN_result/epoch{epoch_num:03}/real/{batch_num:03}.png') \n","    save_img = (gene_img * 127.5 + 127.5)\n","    save_img = PIL.Image.fromarray(np.uint8(save_img))\n","    save_img.save(f'drive/MyDrive/GanStick/ConditionalDCGAN_result/epoch{epoch_num:03}/generated/{batch_num:03}.png')\n","    batch_num += 1\n","  wb.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUioggr6sv04"},"source":["train(future_dataset, historical_dataset, 90, 0)"],"execution_count":null,"outputs":[]}]}