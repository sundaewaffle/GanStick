# -*- coding: utf-8 -*-
"""ver4.4_ConvolutionalBiLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YkwjdgoN7S3XayI61qnjsZn18Jq_YmSo
"""

# install packages for google colab
!pip install XlsxWriter

# dependency
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import os
import PIL
import matplotlib.pyplot as plt
from IPython import display
import time

from openpyxl import load_workbook
from numpy import array
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.preprocessing import StandardScaler
import math

import xlsxwriter

# Import OHLCV datasets
# For this file, I only import Amazon and Apple
# While training, only use one stock, and seprated into training and testing set
def dateClass(ws):
    maxrow = 42831
    date = []
    for r in range(2, maxrow + 1):
        date.append(ws[f'A{r}'].value)
    return date

def openClass(ws):
    maxrow = 42831
    open = []
    for r in range(2, maxrow + 1):
        open.append(ws[f'B{r}'].value)
    return open

def closeClass(ws):
    maxrow = 42831
    close = []
    for r in range(2, maxrow + 1):
        close.append(ws[f'E{r}'].value)
    return close

def highClass(ws):
    maxrow = 42831
    high = []
    for r in range(2, maxrow + 1):
        high.append(ws[f'C{r}'].value)
    return high

def lowClass(ws):
    maxrow = 42831
    low = []
    for r in range(2, maxrow + 1):
        low.append(ws[f'D{r}'].value)
    return low

def volumeClass(ws):
    maxrow = 42831
    low = []
    for r in range(2, maxrow + 1):
        low.append(ws[f'F{r}'].value)
    return low

def OHLCV_dataset(loc, sheet, first, last):
  wb = load_workbook(filename=loc)
  ws = wb[sheet]
  
  data_num = last-first-9
  train_num = math.ceil(data_num * 0.8)

  openPrice = np.array(openClass(ws)[first:last])
  openPrice = openPrice.reshape((len(openPrice), 1))
  open_train = openPrice[0:train_num+9]
  open_test = openPrice[train_num:data_num+9]

  high = np.array(highClass(ws)[first:last])
  high = high.reshape((len(high), 1))
  high_train = high[0:train_num+9]
  high_test = high[train_num:data_num+9]

  low = np.array(lowClass(ws)[first:last])
  low = low.reshape((len(low), 1))
  low_train = low[0:train_num+9]
  low_test = low[train_num:data_num+9]

  close = np.array(closeClass(ws)[first:last])
  close = close.reshape((len(close), 1))
  close_train = close[0:train_num+9]
  close_test = close[train_num:data_num+9]

  volume = np.array(volumeClass(ws)[first:last])
  volume = volume.reshape((len(volume), 1))
  volume_train = volume[0:train_num+9]
  volume_test = volume[train_num:data_num+9]

  trainset = np.hstack((open_train, high_train, low_train, close_train, volume_train))
  testset = np.hstack((open_test, high_test, low_test, close_test, volume_test))
  scaler = StandardScaler()
  scaler.fit(trainset)
  trainset = scaler.transform(trainset)
  testset = scaler.transform(testset)

  return scaler, trainset, testset

# training set is 80% of the stock data, while testing set has 20%
amazon_scaler, amazon_OHLCV_train, amazon_OHLCV_test = OHLCV_dataset('drive/MyDrive/GanStick/Stock.xlsx', 'Amazon', 0, 5695)
apple_scaler, apple_OHLCV_train, apple_OHLCV_test = OHLCV_dataset('drive/MyDrive/GanStick/Stock.xlsx', 'Apple', 2567, 8571)


# load image dataset, use real future images for training for now
BATCH_SIZE = 32

def process(image):
  image = tf.cast((image-127.5) / 127.5 ,tf.float32)
  return image

amazon_images_train = tf.keras.preprocessing.image_dataset_from_directory(
    'drive/MyDrive/GanStick/dataset_eq/future/Amazon',
    batch_size=BATCH_SIZE,
    image_size=(56,56),
    shuffle=False,
    labels=None,
    smart_resize=True,
    validation_split=0.2,
    subset='training'
)
amazon_images_test = tf.keras.preprocessing.image_dataset_from_directory(
    'drive/MyDrive/GanStick/dataset_eq/future/Amazon',
    batch_size=BATCH_SIZE,
    image_size=(56,56),
    shuffle=False,
    labels=None,
    smart_resize=True,
    validation_split=0.2,
    subset='validation'
)
amazon_images_train = amazon_images_train.map(process)
amazon_images_test = amazon_images_test.map(process)

apple_images_train = tf.keras.preprocessing.image_dataset_from_directory(
    'drive/MyDrive/GanStick/dataset_eq/future/Apple',
    batch_size=BATCH_SIZE,
    image_size=(56,56),
    shuffle=False,
    labels=None,
    smart_resize=True,
    validation_split=0.2,
    subset='training'
)
apple_images_test = tf.keras.preprocessing.image_dataset_from_directory(
    'drive/MyDrive/GanStick/dataset_eq/future/Apple',
    batch_size=BATCH_SIZE,
    image_size=(56,56),
    shuffle=False,
    labels=None,
    smart_resize=True,
    validation_split=0.2,
    subset='validation'
)
apple_images_train = apple_images_train.map(process)
apple_images_test = apple_images_test.map(process)


# make models
def make_CNN():
  input_image = layers.Input(shape=(56, 56, 3))

  h = layers.Conv2D(32, (5,5), strides=(2,2), padding='same')(input_image)
  h = layers.BatchNormalization()(h)
  h = layers.LeakyReLU()(h)
  h = layers.Dropout(0.2)(h)

  h = layers.Conv2D(64, (3,3), strides=(1,1), padding='same')(h)
  h = layers.BatchNormalization()(h)
  h = layers.LeakyReLU()(h)
  h = layers.MaxPool2D(pool_size=(2,2), padding='same')(h)
  h = layers.Dropout(0.2)(h)

  h = layers.Conv2D(128, (3,3), strides=(1,1), padding='same')(h)
  h = layers.BatchNormalization()(h)
  h = layers.LeakyReLU()(h)
  h = layers.MaxPool2D(pool_size=(2,2), padding='same')(h)
  h = layers.Dropout(0.2)(h)

  h = layers.Flatten()(h)
  output = layers.Dense(10)(h)

  cnn = keras.Model(inputs=input_image, outputs=output)

  return cnn


def make_BiLSTM(cnn):
  input_feature = layers.Input(shape=(10))
  input_OHLCV = layers.Input(shape=(5, 5))

  reshape_feature = layers.Reshape((1, 10))(input_feature)
  feature = layers.Concatenate(axis=1)([reshape_feature, reshape_feature, reshape_feature, reshape_feature, reshape_feature])
  feature = layers.Concatenate(axis=2)([feature, input_OHLCV])

  seq_output = layers.Bidirectional(layers.LSTM(20, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))(feature)
  feature = layers.BatchNormalization()(seq_output)

  seq_output = layers.Bidirectional(layers.LSTM(50, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))(feature)
  feature = layers.BatchNormalization()(seq_output)

  seq_output = layers.Bidirectional(layers.LSTM(80, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))(feature)
  feature = layers.BatchNormalization()(seq_output)

  feature = layers.TimeDistributed(layers.Dense(10))(feature)
  lstm_output = layers.TimeDistributed(layers.Dense(1))(feature)

  model = keras.Model(inputs=[input_feature, input_OHLCV], outputs=lstm_output)

  return model

CNN = make_CNN()
BiLSTM = make_BiLSTM(CNN)


# Loss function is Mean Squared Error
# I tried to use both SGD and Adam, but the result did not change a lot
mean_squared_error = keras.losses.MeanSquaredError()
CNN_optimizer = keras.optimizers.SGD(learning_rate=0.01)
BiLSTM_optimizer = keras.optimizers.SGD(learning_rate=0.002)
#CNN_optimizer = keras.optimizers.Adam(learning_rate=0.01)
#BiLSTM_optimizer = keras.optimizers.Adam(learning_rate=0.002)


# set checkpoint loaction and load checkpoint
checkpoint_dir = 'drive/MyDrive/GanStick/BiLSTM/BiLSTM_4.4/Amazon'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(CNN_optimizer=CNN_optimizer,
                  BiLSTM_optimizer=BiLSTM_optimizer,
                  CNN=CNN,
                  BiLSTM=BiLSTM)
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))


# training
@tf.function
def train_step(images, input_OHLCVs, real_future_prices, current_batch_size):
  with tf.GradientTape() as CNN_tape, tf.GradientTape() as BiLSTM_tape:
    feature = CNN(images, training=True)
    predict_prices = BiLSTM([feature, input_OHLCVs], training=True)
    loss = mean_squared_error(real_future_prices, predict_prices)
  
  grad_CNN = CNN_tape.gradient(loss, CNN.trainable_variables)
  CNN_optimizer.apply_gradients(zip(grad_CNN, CNN.trainable_variables))

  grad_BiLSTM = BiLSTM_tape.gradient(loss, BiLSTM.trainable_variables)
  BiLSTM_optimizer.apply_gradients(zip(grad_BiLSTM, BiLSTM.trainable_variables))

  # random pick a record to return
  r = np.random.randint(current_batch_size)

  return loss, predict_prices[r], real_future_prices[r]


def train(image_dataset, OHLCV_dataset, scaler, stock, epochs, last_epoch):
  for epoch in range(epochs):
    start_time = time.time()
    recent_epoch = epoch + last_epoch + 1
    data = []
    print('Start training for epoch {}'.format(recent_epoch))

    batch_num = 0

    for images in image_dataset:
      current_batch_size = images.shape[0]
      start = batch_num * BATCH_SIZE
      batch_num += 1

      input_OHLCVs = []
      real_future_prices = []
      for i in range(current_batch_size):
        input_OHLCV = []
        input_OHLCV.append(OHLCV_dataset[start+i])
        input_OHLCV.append(OHLCV_dataset[start+i+1])
        input_OHLCV.append(OHLCV_dataset[start+i+2])
        input_OHLCV.append(OHLCV_dataset[start+i+3])
        input_OHLCV.append(OHLCV_dataset[start+i+4])
        real_future_price = []
        real_future_price.append(OHLCV_dataset[start+i+5][3])
        real_future_price.append(OHLCV_dataset[start+i+6][3])
        real_future_price.append(OHLCV_dataset[start+i+7][3])
        real_future_price.append(OHLCV_dataset[start+i+8][3])
        real_future_price.append(OHLCV_dataset[start+i+9][3])
        input_OHLCVs.append(input_OHLCV)
        real_future_prices.append(real_future_price)
      input_OHLCVs = np.array(input_OHLCVs)
      real_future_prices = np.array(real_future_prices)

      # train step
      loss, predict_prices, real_prices = train_step(images, input_OHLCVs, real_future_prices, current_batch_size)
        
      # store data - random choose batch and timestep
      r = np.random.randint(10)
      if r == 0:
        daily_loss = mean_squared_error(real_prices, predict_prices)
        daily_loss = daily_loss.numpy()
        predict_prices = np.reshape(predict_prices, 5)
        real_prices = real_prices.numpy()
        predict_prices = rescale_price(predict_prices, scaler)
        real_prices = rescale_price(real_prices, scaler)
        print('Predict Prices:', predict_prices)
        print('Real Prices:', real_prices)
        print('Loss for this predict:', daily_loss)
        print('Batch Loss:', loss.numpy())
        print('------------------------------')
        data.append([predict_prices, real_prices, daily_loss])

    #end epochs  
    if (recent_epoch) % 10 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)
    
      os.makedirs(f'drive/MyDrive/GanStick/BiLSTM/Result_ver4.4_BiLSTM/{stock}', exist_ok=True)
      wb = xlsxwriter.Workbook(f'drive/MyDrive/GanStick/BiLSTM/Result_ver4.4_BiLSTM/{stock}/epoch{recent_epoch:03}.xlsx')
      ws = wb.add_worksheet()
      ws.write_row(0, 0, ('Predict 1','Predict 2','Predict 3','Predict 4','Predict 5','Real 1','Real 2','Real 3','Real 4','Real 5','Loss'))
      for i in range(len(data)):
        write_data = data[i]
        ws.write_row(i+1, 0, (write_data[0][0],write_data[0][1],write_data[0][2],write_data[0][3],write_data[0][4],
                    write_data[1][0],write_data[1][1],write_data[1][2],write_data[1][3],write_data[1][4],write_data[2]))
      wb.close()

    display.clear_output(wait=True)
    print('Time for epoch {} is {} sec'.format(recent_epoch, time.time()-start_time))


# rescale the prices
def rescale_price(prices, scaler):
  output_array = []
  for price in prices:
    temp_array = [0, 0, 0, price, 0]
    output = scaler.inverse_transform([temp_array])
    output_array.append(np.around(output[0][3],2))
  return output_array

train(amazon_images_train, amazon_OHLCV_train, amazon_scaler, 'Amazon', 300, 0)

# testing
def test(imageset, OHLCVset, scaler, stock, num):
  batch_num = 0
  save_data = []
  for images in imageset:
    current_batch_size = images.shape[0]
    start = batch_num * BATCH_SIZE
    batch_num += 1

    input_OHLCVs = []
    real_future_prices = []
    for i in range(current_batch_size):
      input_OHLCV = []
      input_OHLCV.append(OHLCVset[start+i])
      input_OHLCV.append(OHLCVset[start+i+1])
      input_OHLCV.append(OHLCVset[start+i+2])
      input_OHLCV.append(OHLCVset[start+i+3])
      input_OHLCV.append(OHLCVset[start+i+4])
      real_future_price = []
      real_future_price.append(OHLCVset[start+i+5][3])
      real_future_price.append(OHLCVset[start+i+6][3])
      real_future_price.append(OHLCVset[start+i+7][3])
      real_future_price.append(OHLCVset[start+i+8][3])
      real_future_price.append(OHLCVset[start+i+9][3])
      input_OHLCVs.append(input_OHLCV)
      real_future_prices.append(real_future_price)
    input_OHLCVs = np.array(input_OHLCVs)
    real_future_prices = np.array(real_future_prices)

    feature = CNN(images)
    predict_prices = BiLSTM([feature, input_OHLCVs])
    predict_prices = np.reshape(predict_prices, (current_batch_size, 5))
    loss = mean_squared_error(real_future_prices, predict_prices)

    print('batch {} loss value: {}'.format(batch_num, loss))
    for i in range(len(real_future_prices)):
      predict_price = predict_prices[i]
      real_future_price = real_future_prices[i]
      predict_price = rescale_price(predict_price, scaler)
      real_future_price = rescale_price(real_future_price, scaler)
      print('{} - Predict: {}, Real: {}'.format(i, predict_price, real_future_price))
      save_data.append([predict_price, real_future_price])

  os.makedirs(f'drive/MyDrive/GanStick/BiLSTM/Result_ver4.4_BiLSTM', exist_ok=True)
  wb = xlsxwriter.Workbook(f'drive/MyDrive/GanStick/BiLSTM/Result_ver4.4_BiLSTM/{stock}/test_{num:03}.xlsx')
  ws = wb.add_worksheet()
  ws.write_row(0, 0, ('ID','Predict 1','Predict 2','Predict 3','Predict 4','Predict 5','Real 1','Real 2','Real 3','Real 4','Real 5'))
  for i in range(len(save_data)):
    ws.write_row(i+1, 0, (i+1, save_data[i][0][0], save_data[i][0][1], save_data[i][0][2], save_data[i][0][3], save_data[i][0][4],
                    save_data[i][1][0], save_data[i][1][1], save_data[i][1][2], save_data[i][1][3], save_data[i][1][4]))
  wb.close()

test(amazon_images_test, amazon_OHLCV_test, amazon_scaler, 'Amazon', 300)
