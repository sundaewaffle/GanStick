{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SAGAN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-GmAOkq5D7M_"},"source":["!pip install XlsxWriter\n","!pip install tensorflow_addons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4hFDlcSDwjV"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHObboSHDlgS"},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow_addons as tfa\n","from tensorflow_addons.layers import SpectralNormalization as SpectralNorm\n","from keras import layers\n","import numpy as np\n","import os\n","import time\n","import xlsxwriter\n","import PIL\n","from IPython import display\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fCk7K0QzCXVs"},"source":["Unzip dataset file. Just need to do it one time."]},{"cell_type":"code","metadata":{"id":"4_X-0RiC9VgZ"},"source":["#!unzip '/content/drive/MyDrive/train/img_align_celeba.zip' -d '/content/drive/MyDrive/train/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3scwrcouChcK"},"source":["Make models.\n","1. Generator: three deconvolution layers with 5x5 kernel, and strides for 2x2. They are all followed by a batch normalization and leakyReLU.\n","2. Discriminator: three convolution layers with 5x5 kernel, and strides for 2x2. They are followed by a leakyReLU. And, the last two convolution layers will dropout for 0.5\n","3. Self-attention layer is inserted between 2nd-3rd convolution in generator, and 1st-2nd in discriminator, since self-attention performs better in middle to high convolution layer."]},{"cell_type":"code","metadata":{"id":"pwc7mBfE4_IJ"},"source":["def SelfAttention(in_shape, ch, k=8):\n","  height, width, channel = in_shape\n","  x = layers.Input(shape = [height, width, channel])\n","\t\n","  f = layers.Conv2D(ch // k, kernel_size = (1, 1), strides = (1, 1), padding='same', use_bias=True)(x)\n","  f = layers.MaxPooling2D()(f)\n","  f = layers.Reshape((-1, f.shape[-1]))(f)\n","\t\n","  g = layers.Conv2D(ch // k, kernel_size = (1, 1), strides = (1, 1), padding='same', use_bias=True)(x)\n","  g = layers.Reshape((-1, g.shape[-1]))(g)\n","\t\n","  h = layers.Conv2D(ch // 2, kernel_size = (1, 1), strides = (1, 1), padding='same', use_bias=True)(x)\n","  h = layers.MaxPooling2D()(h)\n","  h = layers.Reshape((-1, h.shape[-1]))(h)\n","\t\n","  s = tf.matmul(g, f, transpose_b=True)\n","  s = keras.layers.Softmax()(s)\n","\t\t\n","  o = tf.matmul(s, h)\n","\n","  o = layers.Reshape((height, width, ch // 2))(o)\n","  o = layers.Conv2D(channel, kernel_size = (1, 1), strides = (1, 1), padding='same', use_bias=True)(o)\n","  o = Scalar()(o)\n","  o = o + x\n","\n","  SA = keras.Model(inputs=x, outputs=o)\n","\t\n","  return SA\n","\n","\n","class Scalar(layers.Layer):\n","  def __init__(self):\n","    super(Scalar, self).__init__()\n","\n","  def build(self, input_shape):\n","    self.gamma = tf.Variable(initial_value=tf.zeros(1), trainable=True)\n","    self._trainable_weights=[self.gamma]\n","\n","  def call(self, inputs):\n","    return layers.Rescaling(self.gamma)(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WplyRhAYbP8"},"source":["def make_generator_model():\n","  noise = layers.Input(shape=(100,))\n","\n","  hidden = layers.Dense((256 * 8 * 8))(noise)\n","  hidden = layers.BatchNormalization()(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","  hidden = layers.Reshape((8, 8, 256))(hidden)\n","\n","  hidden = SpectralNorm(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))(hidden)\n","  hidden = layers.BatchNormalization()(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","\n","  hidden = SpectralNorm(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))(hidden)\n","  hidden = layers.BatchNormalization()(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","\n","  # Self-attetion\n","  hidden = SelfAttention((32, 32, 64), 64)(hidden)\n","\n","  hidden = SpectralNorm(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))(hidden)\n","  hidden = layers.BatchNormalization()(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","\n","  image = SpectralNorm(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh', use_bias=False))(hidden)\n","  model = keras.Model(inputs=noise, outputs=image)\n","\n","  return model\n","\n","def make_discriminator_model():\n","  image = layers.Input(shape=(128, 128, 3))\n","\n","  hidden = SpectralNorm(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same', input_shape=[128, 128, 3]))(image)\n","  hidden = layers.LeakyReLU()(hidden)\n","\n","  hidden = SpectralNorm(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","\n","  # Self-attetion\n","  hidden = SelfAttention((32, 32, 64), 64)(hidden)\n","\n","  hidden = SpectralNorm(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","  hidden = layers.Dropout(0.5)(hidden)\n","\n","  hidden = SpectralNorm(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))(hidden)\n","  hidden = layers.LeakyReLU()(hidden)\n","  hidden = layers.Dropout(0.5)(hidden)\n","\n","  feature = layers.Flatten()(hidden)\n","  feature = layers.Dense(1, activation=keras.activations.sigmoid)(feature)\n","  model = keras.Model(inputs=image, outputs=feature)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"umjZEvHAh_3C"},"source":["generator = make_generator_model()\n","generator.summary()\n","discriminator = make_discriminator_model()\n","discriminator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dx2a7mV4DRup"},"source":["Both generator and discriminator use Adam optimizers with beta_1=0 and beta_2=0.9\n","\n","Generator learning rate is 0.0001, while discriminator is 0.0004\n","\n","Last function is Binary Cross Entropy."]},{"cell_type":"code","metadata":{"id":"Lg128EZ1zujG"},"source":["cross_entropy = tf.keras.losses.BinaryCrossentropy()\n","generator_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.0, beta_2=0.9)\n","discriminator_optimizer = tf.keras.optimizers.Adam(0.0004, beta_1=0.0, beta_2=0.9)\n","\n","def generator_loss(fake_output):\n","  return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","def discriminator_loss(real_output, fake_output):\n","  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","  total_loss = real_loss + fake_loss\n","  return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56BJqcjNDnjT"},"source":["Loading checkpoint. This part is used for continuing training process.\n","\n","You can change the storing location at checkpoint_dir."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8qpbDy8zlcH","executionInfo":{"status":"ok","timestamp":1638307050556,"user_tz":480,"elapsed":11342,"user":{"displayName":"Sundae Waffle","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11288271932856194814"}},"outputId":"c8e45450-7110-48ea-d707-0192f853d931"},"source":["checkpoint_dir = 'drive/MyDrive/train/SAGAN_check'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                  discriminator_optimizer=discriminator_optimizer,\n","                  generator=generator,\n","                  discriminator=discriminator)\n","manager = tf.train.CheckpointManager(checkpoint, checkpoint_prefix, max_to_keep=5)\n","checkpoint.restore(manager.latest_checkpoint)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fce083b0950>"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"kJ0tydjODxt2"},"source":["Loading Dataset. Those images will be cropped into 128x128 colored images.\n","\n","Batch size is 256.\n","\n","The first parameter of image_dataset_from_directory is the location of the dataset folder."]},{"cell_type":"code","metadata":{"id":"oQuDluuxeqZg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638307227431,"user_tz":480,"elapsed":168824,"user":{"displayName":"Sundae Waffle","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11288271932856194814"}},"outputId":"cf99062d-fc88-4b9a-e7b7-ef45703b1ad7"},"source":["BATCH_SIZE = 256\n","\n","all_images = tf.keras.preprocessing.image_dataset_from_directory(\n","    'drive/MyDrive/train/img_align_celeba/img',\n","    batch_size=1,\n","    image_size=(218,178),\n","    shuffle=True,\n","    labels=None\n",")\n","\n","def process(image):\n","  image = tf.reshape(image, [1, 218, 178, 3])\n","  image = tf.image.crop_and_resize(image, [[0.14, 0.205, 0.86, 0.795]], [0], [128, 128])\n","  image = tf.cast((image-127.5) / 127.5 ,tf.float32)\n","  return image\n","\n","all_images = all_images.map(process)\n","all_images = all_images.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 50000 files belonging to 1 classes.\n"]}]},{"cell_type":"markdown","metadata":{"id":"_WJ3EooQEQhQ"},"source":["Training process.\n","\n","It will automatically randomly pick and store the generator and discriminator loss, and also a real and generated image every 10 batchs.\n","\n","You can change the folder location using the parameter of save_result function."]},{"cell_type":"code","metadata":{"id":"GtuVkLlfxSao"},"source":["noise_dim = 100\n","\n","@tf.function\n","def train_step(real_images, current_batch_size):\n","  noise = tf.random.normal([current_batch_size, noise_dim])\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    generated_images = generator(noise, training=True)\n","    real_output = discriminator(real_images, training=True)\n","    fake_output = discriminator(generated_images, training=True)\n","    disc_loss = discriminator_loss(real_output, fake_output)\n","    gen_loss = generator_loss(fake_output)\n","  \n","  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","  return gen_loss, disc_loss, real_images, generated_images, fake_output\n","\n","\n","def train(dataset, epochs, last_epoch):\n","  for epoch in range(epochs):\n","    start = time.time()\n","    recent_epoch = epoch + last_epoch + 1\n","    data = []\n","    real_images = []\n","    generated_images = []\n","    batch_num = 0\n","    print('Start training for epoch {}'.format(recent_epoch))\n","\n","    for images in dataset:\n","      current_batch_size = images.shape[0]\n","      images = tf.reshape(images, [current_batch_size, images.shape[2], images.shape[3], images.shape[4]])\n","      \n","      gen_loss, disc_loss, real_image, generated_image, fake_output = train_step(images, current_batch_size)\n","      batch_num += 1\n","      if (batch_num % 10) == 0:\n","        r = np.random.randint(current_batch_size)\n","        data.append((gen_loss.numpy(), disc_loss.numpy(), fake_output.numpy()[r]))\n","        real_images.append(real_image.numpy()[r])\n","        generated_images.append(generated_image.numpy()[r])\n","        print('Batch {} training finished'.format(batch_num))\n","    \n","    if (recent_epoch % 30) == 0:\n","      manager.save()\n","      save_result(data, real_images, generated_images, recent_epoch, 'drive/MyDrive/train/SAGAN_result')\n","    display.clear_output(wait=True)\n","    print ('Time for epoch {} is {} sec'.format(recent_epoch, time.time()-start))\n","    print ('generator loss:', gen_loss.numpy())\n","    print ('disciminator loss:', disc_loss.numpy())\n","\n","\n","def save_result(data, real_images, generated_images, epoch_num, loc):\n","  wb = xlsxwriter.Workbook(f'{loc}/epoch{epoch_num:03}.xlsx')\n","  os.makedirs(f'{loc}/epoch{epoch_num:03}/real', exist_ok=True)\n","  os.makedirs(f'{loc}/epoch{epoch_num:03}/generated', exist_ok=True)\n","  ws = wb.add_worksheet()\n","  ws.write_row(0, 0, ('Batch Index', 'Generator Loss', 'Discriminator Loss', 'Generated Image Prediction'))\n","  batch_num = 1\n","  for result, real_img, gene_img in zip(data, real_images, generated_images):\n","    ws.write_row(batch_num, 0, (batch_num, result[0], result[1], result[2]))\n","    save_img = (real_img * 127.5 + 127.5)\n","    save_img = PIL.Image.fromarray(np.uint8(save_img))\n","    save_img.save(f'{loc}/epoch{epoch_num:03}/real/{batch_num:03}.png') \n","    save_img = (gene_img * 127.5 + 127.5)\n","    save_img = PIL.Image.fromarray(np.uint8(save_img))\n","    save_img.save(f'{loc}/epoch{epoch_num:03}/generated/{batch_num:03}.png')\n","    batch_num += 1\n","  wb.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__U_A-_3Y8Dv"},"source":["train(all_images, 300, 0)"],"execution_count":null,"outputs":[]}]}